{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Funciones Utilizadas Trabajo Fin de Grado\n",
        "\n",
        "# Predicción de los Precios de las Viviendas en Madrid Capital\n",
        "\n",
        "# Grado en Estadística y Empresa - Curso 2023/2024\n",
        "\n",
        "# Andrés Rubio Lafuente\n"
      ],
      "metadata": {
        "id": "lE9AX7o9cp45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este archivo contiene el conjunto de funciones que se utilizan para la realización del Trabajo de Fin de Grado.\n",
        "\n",
        "Cargamos los paquetes necesarios.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cZQkYQundvQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Básico\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Métodos de Aprendizaje\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import KFold, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import r2_score\n",
        "from scipy.cluster.hierarchy import dendrogram\n",
        "from scipy.stats import t\n",
        "\n",
        "# Tiempo de Ejecucion\n",
        "import time\n",
        "\n",
        "# Advertencias\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")"
      ],
      "metadata": {
        "id": "gs0z5nq7dPmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Funcion que elimina los valores nulos y transforma el tipo de variable a numérico."
      ],
      "metadata": {
        "id": "06pZlUeXeQCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nulos(datos, x):\n",
        "  indices = []\n",
        "  valores = []\n",
        "  i = 0\n",
        "  while i < len(datos[x]):\n",
        "\n",
        "      # Vemos que instancias permiten el cambio de dato a tipo numérico.\n",
        "      try:\n",
        "            int(datos[x][i])\n",
        "            str(datos[x][i])\n",
        "\n",
        "      # Las instancias que no lo permiten es porque son de tipo texto o nulos.\n",
        "      except:\n",
        "            indices.append(i)\n",
        "            valores.append(datos[x][i])\n",
        "      i += 1\n",
        "\n",
        "  # Vemos que instancias son de tipo texto o nulos y que valor toman.\n",
        "  valores = set(valores)\n",
        "  print(f\"Las filas a eliminar son: {indices}, pues toman valores {valores}.\")\n",
        "  print(f\"En total se eliminan {len(indices)} filas del conjunto de datos por la variable {x}.\")\n",
        "\n",
        "  # Eliminamos estas instancias que toman valor tipo texto o nulo.\n",
        "  datos.drop(indices, axis = 0, inplace = True)\n",
        "  datos.reset_index(drop = True, inplace = True)\n",
        "\n",
        "  # Cambiamos el tipo de variable a numérico\n",
        "  datos[x] = datos[x].astype(int)\n",
        "  return(datos[x])"
      ],
      "metadata": {
        "id": "I-_PsruXeTlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Función que identifica los valores de tipo texto y los transforma a valores nulos.\n",
        "\n"
      ],
      "metadata": {
        "id": "5eD-RbcIebFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nulos_texto(datos, x):\n",
        "  indices = []\n",
        "  valores = []\n",
        "  i = 0\n",
        "  while i < len(datos[x]):\n",
        "\n",
        "      # Vemos que instancias permiten el cambio de dato a tipo numérico.\n",
        "      try:\n",
        "            int(datos[x][i])\n",
        "\n",
        "      # Las instancias que no lo permiten es porque son de tipo texto.\n",
        "      # Transformamos los valores de tipo texto a valores faltantes.\n",
        "      except:\n",
        "            indices.append(i)\n",
        "            valores.append(datos[x][i])\n",
        "            datos.at[i,x] = np.nan\n",
        "      i += 1\n",
        "\n",
        "  # Vemos que instancias son de tipo texto y que valor toman.\n",
        "  valores = set(valores)\n",
        "  datos[x] = pd.to_numeric(datos[x])\n",
        "  print(f\"Las filas con valores faltantes son : {indices}, pues toman valores {valores}.\")\n",
        "  print(f\"En total hay {len(indices)} filas con valores faltantes del conjunto de datos para la variable {x}.\")\n",
        "  print(f\"La variable {x} es del tipo: {datos.dtypes[x]}\")\n",
        "  return(datos[x])"
      ],
      "metadata": {
        "id": "t_ohJffyeexS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Función para identificar las instancias con valores faltantes.\n",
        "\n"
      ],
      "metadata": {
        "id": "szL2ySnBiDQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def valores_faltantes(datos):\n",
        "\n",
        "  # Para cada variable, se extraen las filas con valores faltantes\n",
        "  ind = []\n",
        "  for i in datos.columns:\n",
        "      if datos.isna().sum()[i] > 0:\n",
        "            ind_inicial = datos[datos.isna()[i] == True].index\n",
        "      ind.append(ind_inicial)\n",
        "\n",
        "  # Dado que una fila puede tener varios valores faltantes, entonces puede aparecer múltiples veces.\n",
        "  # Debemos eliminar los indices que se repitan de forma que un indice solo aparezca 1 vez.\n",
        "  ind_final = set()\n",
        "  for i in ind:\n",
        "      ind_final.update(i.tolist())\n",
        "  return(ind_final)"
      ],
      "metadata": {
        "id": "Te-9XRY2iJzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Funcion para generar diagramas de caja o boxplots."
      ],
      "metadata": {
        "id": "5YFrEfkjALKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def boxplot_num(datos, variable_num, variable_cat, variable_orden):\n",
        "\n",
        "    # Creamos un diagrama de caja o boxplot para una variable numérica en función de una variable categórica.\n",
        "    # Es decir, por ejemplo podemos obtener el boxplot del precio en función del distrito.\n",
        "\n",
        "    plt.figure(figsize = (10, 10))\n",
        "    sns.set(style = 'darkgrid')\n",
        "    orden = datos.groupby(variable_cat)[variable_orden].mean().sort_values(ascending = False).index\n",
        "    sns.boxplot(data = datos, x = variable_cat, y = datos[variable_num], hue = variable_cat, order = orden, hue_order = orden, legend = False)\n",
        "    plt.xticks(rotation = 90)\n",
        "    plt.xlabel(variable_cat)\n",
        "    plt.ylabel(variable_num)\n",
        "    plt.tight_layout()"
      ],
      "metadata": {
        "id": "PWupjvUWAO64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Función que elimina los valores atipicos que son muy extremos.\n"
      ],
      "metadata": {
        "id": "AriIP53PeseF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def valores_atipicos(datos, variable_num, variable_cat):\n",
        "\n",
        "  # Identificamos los datos que corresponden a cada distrito.\n",
        "  datos_categoria = []\n",
        "  valores = datos[variable_cat].unique()\n",
        "  for i in valores:\n",
        "      datos_valores = datos[datos[variable_cat] == i]\n",
        "\n",
        "      # Calculamos el rango intercuartilico.\n",
        "      q1 = datos_valores[variable_num].quantile(0.25)\n",
        "      q3 = datos_valores[variable_num].quantile(0.75)\n",
        "      IQR = q3 - q1\n",
        "\n",
        "      # Definimos los limites inferiores y superiores\n",
        "      lim_inf = q1 - 3 * IQR\n",
        "      lim_sup = q3 + 3 * IQR\n",
        "\n",
        "      # Todos aquellos valores que estén fuera de los límites se eliminan del conjunto de datos.\n",
        "      datos_filtrados = datos_valores[((datos_valores[variable_num] > lim_inf) & (datos_valores[variable_num] < lim_sup)) | (datos_valores[variable_num].isna() == True)]\n",
        "      datos_categoria.append(datos_filtrados)\n",
        "  datos_retorno = pd.concat(datos_categoria)\n",
        "  return(datos_retorno)"
      ],
      "metadata": {
        "id": "lod3fwXUevpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Función que nos permite aplicar ciertos métodos de aprendizaje supervisado con búsqueda en rejilla de hiperparámetros."
      ],
      "metadata": {
        "id": "7tfWX-BLiNia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def regresion_grid(preproceso, metodo, params, X_train, Y_train, X_test, Y_test):\n",
        "\n",
        "  # Definimos el metodo que se va a aplicar.\n",
        "  regresor = Pipeline([('preproceso', preproceso),\n",
        "                       ('regresor', metodo)])\n",
        "\n",
        "  # Para la validación interna (ajuste de hiperparámetros) utilizamos validación cruzada con k = 10 folds.\n",
        "  val_interna = KFold(n_splits = 10, shuffle = True, random_state = 129)\n",
        "\n",
        "  # Empleamos la búsqueda en rejilla y el r2 como métrica de evaluación del modelo.\n",
        "  grid_search = GridSearchCV(estimator = regresor, param_grid = params, cv = val_interna, scoring = 'r2')\n",
        "\n",
        "  # Entrenamos el modelo y calculamos el tiempo de entrenamiento del modelo.\n",
        "  tiempo_inicio = time.time()\n",
        "  grid_search.fit(X_train, Y_train)\n",
        "  tiempo_fin = time.time()\n",
        "  tiempo_ejecucion = tiempo_fin - tiempo_inicio\n",
        "\n",
        "  # Vemos cuáles son los mejores valores de los hiperparámetros así como el r2 de entrenamiento y de evaluación.\n",
        "  mejores_hp = grid_search.best_params_\n",
        "  r2_train = grid_search.best_score_\n",
        "  mejor_modelo = grid_search.best_estimator_\n",
        "  pred = mejor_modelo.predict(X_test)\n",
        "  r2_test = r2_score(Y_test, pred)\n",
        "\n",
        "  # Guardamos todos los resultados en un dataframe.\n",
        "  res = pd.DataFrame([[metodo, mejores_hp, r2_train, r2_test, tiempo_ejecucion]])\n",
        "  res.columns = ['Metodo AS', 'Mejores Hiperparámetros', 'R2 Train', 'R2 Test', 'Tiempo de Ejecución (Segundos)']\n",
        "  return(res)"
      ],
      "metadata": {
        "id": "pUTGJUdPiUvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Función que nos permite aplicar ciertos métodos de aprendizaje supervisado con búsqueda aleatoria de hiperparámetros."
      ],
      "metadata": {
        "id": "y8oRRrR1ihOM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2e05s0OIcoT2"
      },
      "outputs": [],
      "source": [
        "def regresion_random(preproceso, metodo, params, X_train, Y_train, X_test, Y_test):\n",
        "\n",
        "  # Definimos el metodo que se va a aplicar.\n",
        "  regresor = Pipeline([('preproceso', preproceso),\n",
        "                       ('regresor', metodo)])\n",
        "\n",
        "  # Para la validación interna (ajuste de hiperparámetros) utilizamos validación cruzada con k = 10 folds.\n",
        "  val_interna = KFold(n_splits = 10, shuffle = True, random_state = 129)\n",
        "\n",
        "  # Empleamos la búsqueda en rejilla y el r2 como métrica de evaluación del modelo.\n",
        "  random_search = RandomizedSearchCV(regresor, params, cv = val_interna, scoring = 'r2', random_state = 129)\n",
        "\n",
        "  # Entrenamos el modelo y calculamos el tiempo de entrenamiento del modelo.\n",
        "  tiempo_inicio = time.time()\n",
        "  random_search.fit(X_train, Y_train)\n",
        "  tiempo_fin = time.time()\n",
        "  tiempo_ejecucion = tiempo_fin - tiempo_inicio\n",
        "\n",
        "  # Vemos cuáles son los mejores valores de los hiperparámetros así como el r2 de entrenamiento y de evaluación.\n",
        "  mejores_hp = random_search.best_params_\n",
        "  r2_train = random_search.best_score_\n",
        "  mejor_modelo = random_search.best_estimator_\n",
        "  pred = mejor_modelo.predict(X_test)\n",
        "  r2_test = r2_score(Y_test, pred)\n",
        "\n",
        "  # Guardamos todos los resultados en un dataframe.\n",
        "  res = pd.DataFrame([[metodo, mejores_hp, r2_train, r2_test, tiempo_ejecucion]])\n",
        "  res.columns = ['Metodo AS', 'Mejores Hiperparámetros', 'R2 Train', 'R2 Test', 'Tiempo de Ejecución (Segundos)']\n",
        "  return(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Función para Redes Neuronales para búsqueda de hiperparámetros y entrenamiento del modelo"
      ],
      "metadata": {
        "id": "zzW1pVseRthl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def redes_neuronales(preproceso, metodo, params, X_train, Y_train, X_test, Y_test):\n",
        "\n",
        "  # Definimos el metodo que se va a aplicar.\n",
        "  regresor = Pipeline([('preproceso', preproceso),\n",
        "                       ('regresor', metodo)])\n",
        "\n",
        "  # Empleamos la búsqueda en rejilla y el r2 como métrica de evaluación del modelo.\n",
        "  random_search = RandomizedSearchCV(regresor, params, scoring = 'r2', random_state = 129)\n",
        "\n",
        "  # Entrenamos el modelo y calculamos el tiempo de entrenamiento del modelo.\n",
        "  tiempo_inicio = time.time()\n",
        "  random_search.fit(X_train, Y_train)\n",
        "  tiempo_fin = time.time()\n",
        "  tiempo_ejecucion = tiempo_fin - tiempo_inicio\n",
        "\n",
        "  # Vemos cuáles son los mejores valores de los hiperparámetros así como el r2 de entrenamiento y de evaluación.\n",
        "  mejores_hp = random_search.best_params_\n",
        "  r2_train = random_search.best_score_\n",
        "  mejor_modelo = random_search.best_estimator_\n",
        "  pred = mejor_modelo.predict(X_test)\n",
        "  r2_test = r2_score(Y_test, pred)\n",
        "\n",
        "  # Guardamos todos los resultados en un dataframe.\n",
        "  res = pd.DataFrame([[metodo, mejores_hp, r2_train, r2_test, tiempo_ejecucion]])\n",
        "  res.columns = ['Metodo AS', 'Mejores Hiperparámetros', 'R2 Train', 'R2 Test', 'Tiempo de Ejecución (Segundos)']\n",
        "  return(res)"
      ],
      "metadata": {
        "id": "WIgjMVnNRucZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Función que dibuja un dendrograma."
      ],
      "metadata": {
        "id": "Q3nIFGa4QYQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dendrograma(model, **kwargs):\n",
        "    plt.figure(figsize = (20, 12))\n",
        "    cont = np.zeros(model.children_.shape[0])\n",
        "    n = len(model.labels_)\n",
        "    for i, j in enumerate(model.children_):\n",
        "        cont1 = 0\n",
        "        for idx in j:\n",
        "            if idx < n:\n",
        "                cont += 1\n",
        "            else:\n",
        "                cont1 += cont[idx - n]\n",
        "        cont[i] = cont1\n",
        "    linkage_matrix = np.column_stack([model.children_, model.distances_, cont]).astype(float)\n",
        "    _ = dendrogram(linkage_matrix, **kwargs)"
      ],
      "metadata": {
        "id": "RmKm7TuARLjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Función que calcula el intervalo de confianza para la media basado en la distribución t de Student."
      ],
      "metadata": {
        "id": "WcgC-Iseqqbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def intervalo_confianza(n, media, desv, conf_level):\n",
        "\n",
        "  # Obtenemos la desviación típica (se), el valor del estadístico T-Student y los límites inferior y superior\n",
        "  se = desv / np.sqrt(n)\n",
        "  est = t.ppf((1 + conf_level) / 2, n-1)\n",
        "  inf = media - est * se\n",
        "  sup = media + est * se\n",
        "\n",
        "  # Aplicamos los formatos a las variables para que tengan 2 decimales y guardamos los datos en un dataframe\n",
        "  X_medias_f = media.astype(float).round(2).apply(lambda x: '{:0g}'.format(x))\n",
        "  inf_f = inf.astype(float).round(2).apply(lambda x: '{:0g}'.format(x))\n",
        "  sup_f = sup.astype(float).round(2).apply(lambda x: '{:0g}'.format(x))\n",
        "  pd.options.display.float_format = '{:,.0f}'.format\n",
        "  ic = pd.DataFrame(pd.concat([X_medias_f, inf_f, sup_f], axis = 1, keys = ['Media', 'IC Inferior', 'IC Superior']))\n",
        "  return(ic)"
      ],
      "metadata": {
        "id": "hjh23H8vquCd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}